{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import re\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# tokens=text_to_word_sequence(\"manta.com/c/mmcdqky/lily-co\")\n",
    "\n",
    "# print(tokens)\n",
    "\n",
    "\n",
    "# #to map the features to a dictioanary and then convert it to a csv file.\n",
    "# # Feauture extraction \n",
    "# class feature_extractor(object):\n",
    "#     def __init__(self,url):\n",
    "#         self.url=url\n",
    "#         self.length=len(url)\n",
    "#         #self.domain=url.split('//')[-1].split('/')[0]\n",
    "#     #def entropy(self):\n",
    "#     #.com,.org,.net,.edu\n",
    "#     #has www.\n",
    "#     #.extension-- .htm,.html,.php,.js\n",
    "#     # Pattern regex = Pattern.compile(\".com[,/.]\")\n",
    "#     def domain(self):\n",
    "#         if re.search(\".com[ .,/]\",self.url):\n",
    "#             return 1\n",
    "#         elif re.search(\".org[.,/]\",self.url):\n",
    "#             return 2\n",
    "#         elif re.search(\".net[.,/]\",self.url):\n",
    "#             return 3\n",
    "#         elif re.search(\".edu[.,/]\",self.url):\n",
    "#             return 4\n",
    "#         else:\n",
    "#             return 0\n",
    "#     #def extension(self):\n",
    "\n",
    "#     def num_digits(self):\n",
    "#         return sum(n.isdigit() for n in self.url)\n",
    "    \n",
    "#     def num_char(self):\n",
    "#         return sum(n.alpha() for n in self.url)\n",
    "    \n",
    "#     def has_http(self):\n",
    "#         if \"http\" in self.url:\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "    \n",
    "#     def has_https(self):\n",
    "#         if \"https\" in self.url:\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "    \n",
    "#     #def num_special_char(self):\n",
    "#     #\n",
    "    \n",
    "#     #def num\n",
    "\n",
    "\n",
    "\n",
    "# def clean(input):\n",
    "#     tokensBySlash = str(input.encode('utf-8')).split('/')\n",
    "#     allTokens=[]\n",
    "#     for i in tokensBySlash:\n",
    "#         tokens = str(i).split('-')\n",
    "#         tokensByDot = []\n",
    "#         for j in range(0,len(tokens)):\n",
    "#             tempTokens = str(tokens[j]).split('.')\n",
    "#             tokentsByDot = tokensByDot + tempTokens\n",
    "#         allTokens = allTokens + tokens + tokensByDot\n",
    "#     allTokens = list(set(allTokens))\n",
    "#     if 'com' in allTokens:\n",
    "#         allTokens.remove('com')\n",
    "#     return allTokens"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['manta', 'com', 'c', 'mmcdqky', 'lily', 'co']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.parse import urlparse\n",
    "url=\"http://www.pn-wuppertal.de/links/2-linkseite/5-httpwwwkrebshilfede\"\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def getTokens(input):\n",
    "    tokensBySlash = str(input.encode('utf-8')).split('/')\n",
    "    allTokens=[]\n",
    "    for i in tokensBySlash:\n",
    "        tokens = str(i).split('-')\n",
    "        tokensByDot = []\n",
    "        for j in range(0,len(tokens)):\n",
    "            tempTokens = str(tokens[j]).split('.')\n",
    "            tokentsByDot = tokensByDot + tempTokens\n",
    "        allTokens = allTokens + tokens + tokensByDot\n",
    "    allTokens = list(set(allTokens))\n",
    "    if 'com' in allTokens:\n",
    "        allTokens.remove('com')\n",
    "    return allTokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "url=\"http://www.pn-wuppertal.de/links/2-linkseite/5-httpwwwkrebshilfede\"\n",
    "x=(lambda s: sum(not((i.isalpha()) and not(i.isnumeric())) for i in s))\n",
    "print(x(url))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from urllib.parse import urlparse\n",
    "url=\"http://www.pn-wuppertal.de/links/2-linkseite/5-httpwwwkrebshilfede\"\n",
    "def fd_length(url):\n",
    "    urlpath= urlparse(url).path\n",
    "    try:\n",
    "        return len(urlpath.split('/')[1])\n",
    "    except:\n",
    "        return 0\n",
    "print(urlparse(url))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ParseResult(scheme='http', netloc='www.pn-wuppertal.de', path='/links/2-linkseite/5-httpwwwkrebshilfede', params='', query='', fragment='')\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)"
  },
  "interpreter": {
   "hash": "96e5d53265b94df340949bd6ef5afdc360b59f503fc29fdfbde1773c536f468a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}